{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Practical Session 2\n",
    "\n",
    "\n",
    "The goal of this practical session is to implement Value Iteration, Policy Iteration and Q-Learning for a GridWorld environment.\n",
    "\n",
    "* Exercise 1: implement of the Bellman operator $T^*$ and verify its contraction property.\n",
    "* Exercise 2: value iteration.\n",
    "* Exercise 3: policy iteration.\n",
    "* Exercise 4: Q-Learning.\n",
    "\n",
    "\n",
    "## Review\n",
    "\n",
    "A Markov Decision Process (MDP) is defined as tuple $(S, A, P, r, \\gamma)$ where:\n",
    "* $S$ is the state space\n",
    "* $A$ is the action space \n",
    "* $P$ represents the transition probabilities, $P(s,a,s')$ is the probability of arriving at state $s'$ by taking action $a$ in state $s$\n",
    "* $r$ is the reward function such that $r(s,a,s')$ is the reward obtained by taking action $a$ in state $s$ and arriving at $s'$\n",
    "* $\\gamma$ is the discount factor\n",
    "\n",
    "A deterministic policy $\\pi$ is a mapping from $S$ to $A$: $\\pi(s)$ is the action to be taken at state $s$.\n",
    "\n",
    "The goal of an agent is to find the policy $\\pi$ that maximizes the expected sum of discounted rewards by following $\\pi$. The value of $\\pi$ is defined as\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = E\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t, S_{t+1}) | S_0 = s \\right]\n",
    "$$\n",
    "\n",
    "$V_\\pi(s)$ and the optimal value function, defined as $V^*(s) = \\max_\\pi V_\\pi(s)$, can be shown to satisfy the Bellman equations:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_{s' \\in S}  P(s,\\pi(s),s')[r(s,\\pi(s),s') + \\gamma V_\\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "It is sometimes better to work with Q functions:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma  Q_\\pi(s', \\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma \\max_{a'} Q^*(s', a')]\n",
    "$$\n",
    "\n",
    "such that $V_\\pi(s) = Q_\\pi(s, \\pi(s))$ and $V^*(s) = \\max_a Q^*(s, a)$.\n",
    "\n",
    "\n",
    "### Using value iteration to compute an optimal policy\n",
    "If the reward function and the transition probabilities are known (and the state and action spaces are not very large), we can use dynamic programming methods to compute $V^*(s)$. Value iteration is one way to do that.\n",
    "\n",
    "\n",
    "#####  Value iteration to compute $V^*(s)$\n",
    "$$\n",
    "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')]   \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "* For any $Q_0$, let $Q_n = T^* Q_{n-1}$. \n",
    "* We have $\\lim_{n\\to\\infty}Q_n = Q^*$ and $Q^* = T^* Q^*$\n",
    "\n",
    "\n",
    "##### Finding the optimal policy from $V^\\pi(s)$\n",
    "\n",
    "The optimal policy $\\pi^*$ can be computed as\n",
    "\n",
    "$$\n",
    "\\pi^*(s) \\in \\arg\\max_{a\\in A} Q^*(s, a) =  \\arg\\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.envs import ToyEnv1, SimpleGridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can use a simpler environment, __ToyEnv1__ to debug your algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Set of actions: [0, 1, 2, 3]\n",
      "Number of states:  18\n",
      "Number of actions:  4\n",
      "P has shape:  (18, 4, 18)\n",
      "discount factor:  0.95\n",
      "\n",
      "initial state:  0\n",
      "reward at (s=1, a=3,s'=2):  -1\n",
      "\n",
      "random policy =  [1 3 2 2 0 3 0 1 3 1 3 3 3 3 3 2 1 3]\n",
      "(s, a, s', r):\n",
      "0 1 1 -1\n",
      "1 3 7 0.0\n",
      "7 1 8 0.0\n",
      "8 3 14 0.0\n",
      "\n",
      "Visualization:\n",
      "o  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  A  o  o  o \n",
      "\n",
      "(15, 0.0, False, {})\n",
      "o  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  A  o  o \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small environment for debugging\n",
    "env = SimpleGridWorld(gamma=0.95, success_probability=1.0)\n",
    "# env = ToyEnv1(gamma=0.95)\n",
    "\n",
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=1, a=3,s'=2): \", env.reward_fn(1,3,2))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward)\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n",
    "\n",
    "# Visualizing the environment\n",
    "try:\n",
    "    print(\"Visualization:\")\n",
    "    env.render()\n",
    "except:\n",
    "    pass # render not available\n",
    "\n",
    "print(env.step(1))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Bellman operator\n",
    "\n",
    "1. Write a function that takes an environment and a state-action value function $Q$ as input and returns the Bellman optimality operator applied to $Q$, $T^* Q$ and the greedy policy with respect to $Q$.\n",
    "3. Let $Q_1$ and $Q_2$ be state-action value functions. Verify the contraction property:  $\\Vert T^* Q_1 - T^* Q_2\\Vert \\leq \\gamma ||Q_1 - Q_2||$, where $||Q|| = \\max_{s,a} |Q(s,a)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Your answer to 1.\n",
    "# --------------\n",
    "def bellman_operator(Q, env):\n",
    "    TQ = 0\n",
    "    greedy_policy = []\n",
    "    ###\n",
    "    # To fill\n",
    "    ###\n",
    "    return TQ, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction of Bellman operator\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Your answer to 2.\n",
    "# --------------\n",
    "print(\"Contraction of Bellman operator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Value iteration (VI)\n",
    "\n",
    "1. (Optimal Value function) Write a function that takes as input an initial state-action value function `Q0` and an environment `env` and returns a vector `Q` such that $||T^* Q -  Q ||_\\infty \\leq \\varepsilon $ and the greedy policy with respect to $Q$.\n",
    "2. Test the convergence of the function you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Your answer to 1.\n",
    "# --------------\n",
    "def value_iteration(Q0, env, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finding the optimal value function. To be done!\n",
    "    \"\"\"\n",
    "    TQ = 0\n",
    "    greedy_policy = []\n",
    "    return TQ, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Your answer to 2.\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Policy iteration (PI)\n",
    "\n",
    "Policy iteration is another algorithm to find an optimal policy when the MDP is known:\n",
    "\n",
    "$$\n",
    "\\pi_{n} \\gets \\mathrm{greedy}(V_{\\pi_{n-1}}) \\\\\n",
    "V_{\\pi_n} \\gets \\mbox{policy-evaluation}(\\pi_n)\n",
    "$$\n",
    "For any arbitrary $\\pi_0$, $\\pi_n$ converges to $\\pi^*$.\n",
    "\n",
    "Implement policy iteration and compare it to value iteration.\n",
    "\n",
    "\n",
    "### Exact policy evaluation\n",
    "\n",
    "Each iteration of PI requires a policy evaluation. This step can be done with value iteration, but the value of a policy can also be computed exactly by solving a linear system: $(I - \\gamma P_\\pi) V^\\pi = r_\\pi $ where $P_\\pi$ is the transition kernel (matrix) induced by $\\pi$: $P_\\pi(s, s') = P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ and $r_\\pi(s') = \\sum_{s} r(s,\\pi(s),s') P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ is the average reward obtained in state $s'$ by following the policy $\\pi$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_policy_eval(pi, env):\n",
    "    \"\"\"\n",
    "    To be done!\n",
    "    \"\"\"\n",
    "    # Compute the transition matrix P_pi and reward vector r_pi\n",
    "    P_pi = np.zeros((env.Ns, env.Ns))\n",
    "    r_pi = np.zeros(env.Ns)\n",
    "    \n",
    "    #\n",
    "    # ...\n",
    "    #\n",
    "    \n",
    "    A = np.eye(env.Ns) - env.gamma*P_pi\n",
    "    b = r_pi\n",
    "\n",
    "    # Solve linear system\n",
    "    V = np.linalg.solve(A, b)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(pi0, env):\n",
    "    \"\"\"\n",
    "    Finding the optimal policy. To be done!\n",
    "    \"\"\"\n",
    "    pi = np.zeros(env.Ns)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Q-Learning\n",
    "\n",
    "\n",
    "###  Q-Learning \n",
    "\n",
    "When the reward function and the transition probabilities are *unknown*, we cannot use dynamic programming to find the optimal value function. Q-Learning is a stochastic approximation algorithm that allows us to estimate the value function by using only samples from the environment.\n",
    "\n",
    "Q-learning with $\\varepsilon$-greedy exploration does the following update at time $t$:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}  $\n",
    "\n",
    "\n",
    "\n",
    "#####  Q-learning\n",
    "\n",
    "The Q-Learning algorithm allows us to estimate the optimal Q function using only trajectories from the MDP obtained by following some exploration policy. \n",
    "\n",
    "Q-learning with $\\varepsilon$-greedy exploration does the following update at time $t$:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$ (**act function**);\n",
    "2. Observe $s_{t+1}$ and reward $r_t$ (**step in the environment**);\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$ (**to be done in .update()**) ;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$ (**in update() too**)\n",
    "\n",
    "\n",
    "Implement Q-learning and test its convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Q-Learning implementation\n",
    "# ------------------------------\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Implements Q-learning algorithm with epsilon-greedy exploration\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, learning, epsilon): # You can add more argument to your init (lr decay, eps decay)\n",
    "        pass\n",
    "    \n",
    "    def act(self, state, greedy=False): # You don't have to use this template for your algorithm, those are just hints\n",
    "        \"\"\"\n",
    "        Takes a state as input and outputs an action (acting greedily or not with respect to the q function)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, state, action_taken, next_state, reward):\n",
    "        \"\"\"\n",
    "        Takes (s, a, s', r) as input and update the Q function\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Convergence of Q-Learning\n",
    "# ---------------------------\n",
    "\n",
    "# Number of Q learning iterations\n",
    "n_steps = int(1e5)  \n",
    "#n_steps = 10\n",
    "\n",
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "# You can use Q_opt from value iteration to check the correctness of q learning\n",
    "Q_opt, pi_opt = value_iteration(Q0, env, epsilon=1e-6)\n",
    "#       ^ and the optimal policy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
