{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Practical Session 2\n",
    "\n",
    "\n",
    "The goal of this practical session is to implement Value Iteration, Policy Iteration and Q-Learning for a GridWorld environment.\n",
    "\n",
    "* Exercise 1: implement of the Bellman operator $T^*$ and verify its contraction property.\n",
    "* Exercise 2: value iteration.\n",
    "* Exercise 3: policy iteration.\n",
    "* Exercise 4: Q-Learning.\n",
    "\n",
    "\n",
    "## Review\n",
    "\n",
    "A Markov Decision Process (MDP) is defined as tuple $(S, A, P, r, \\gamma)$ where:\n",
    "* $S$ is the state space\n",
    "* $A$ is the action space \n",
    "* $P$ represents the transition probabilities, $P(s,a,s')$ is the probability of arriving at state $s'$ by taking action $a$ in state $s$\n",
    "* $r$ is the reward function such that $r(s,a,s')$ is the reward obtained by taking action $a$ in state $s$ and arriving at $s'$\n",
    "* $\\gamma$ is the discount factor\n",
    "\n",
    "A deterministic policy $\\pi$ is a mapping from $S$ to $A$: $\\pi(s)$ is the action to be taken at state $s$.\n",
    "\n",
    "The goal of an agent is to find the policy $\\pi$ that maximizes the expected sum of discounted rewards by following $\\pi$. The value of $\\pi$ is defined as\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = E\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t, S_{t+1}) | S_0 = s \\right]\n",
    "$$\n",
    "\n",
    "$V_\\pi(s)$ and the optimal value function, defined as $V^*(s) = \\max_\\pi V_\\pi(s)$, can be shown to satisfy the Bellman equations:\n",
    "\n",
    "$$\n",
    "V_\\pi(s) = \\sum_{s' \\in S}  P(s,\\pi(s),s')[r(s,\\pi(s),s') + \\gamma V_\\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "It is sometimes better to work with Q functions:\n",
    "\n",
    "$$\n",
    "Q_\\pi(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma  Q_\\pi(s', \\pi(s')]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma \\max_{a'} Q^*(s', a')]\n",
    "$$\n",
    "\n",
    "such that $V_\\pi(s) = Q_\\pi(s, \\pi(s))$ and $V^*(s) = \\max_a Q^*(s, a)$.\n",
    "\n",
    "\n",
    "### Using value iteration to compute an optimal policy\n",
    "If the reward function and the transition probabilities are known (and the state and action spaces are not very large), we can use dynamic programming methods to compute $V^*(s)$. Value iteration is one way to do that.\n",
    "\n",
    "\n",
    "#####  Value iteration to compute $V^*(s)$\n",
    "$$\n",
    "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')]   \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "* For any $Q_0$, let $Q_n = T^* Q_{n-1}$. \n",
    "* We have $\\lim_{n\\to\\infty}Q_n = Q^*$ and $Q^* = T^* Q^*$\n",
    "\n",
    "\n",
    "##### Finding the optimal policy from $V^\\pi(s)$\n",
    "\n",
    "The optimal policy $\\pi^*$ can be computed as\n",
    "\n",
    "$$\n",
    "\\pi^*(s) \\in \\arg\\max_{a\\in A} Q^*(s, a) =  \\arg\\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('utils')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.envs import ToyEnv1, SimpleGridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can use a simpler environment, __ToyEnv1__ to debug your algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Set of actions: [0, 1, 2, 3]\n",
      "Number of states:  18\n",
      "Number of actions:  4\n",
      "P has shape:  (18, 4, 18)\n",
      "discount factor:  0.95\n",
      "\n",
      "initial state:  0\n",
      "reward at (s=1, a=3,s'=2):  -1\n",
      "\n",
      "random policy =  [0 1 2 0 3 0 2 0 2 3 3 3 2 1 0 3 3 3]\n",
      "(s, a, s', r):\n",
      "0 0 0 0.0\n",
      "0 0 0 0.0\n",
      "0 0 0 0.0\n",
      "0 0 0 0.0\n",
      "\n",
      "Visualization:\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "(1, -1, False, {})\n",
      "o  A  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small environment for debugging\n",
    "env = SimpleGridWorld(gamma=0.95, success_probability=1.0)\n",
    "# env = ToyEnv1(gamma=0.95)\n",
    "\n",
    "# Useful attributes\n",
    "print(\"Set of states:\", env.states)\n",
    "print(\"Set of actions:\", env.actions)\n",
    "print(\"Number of states: \", env.Ns)\n",
    "print(\"Number of actions: \", env.Na)\n",
    "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
    "print(\"discount factor: \", env.gamma)\n",
    "print(\"\")\n",
    "\n",
    "# Usefult methods\n",
    "state = env.reset() # get initial state\n",
    "print(\"initial state: \", state)\n",
    "print(\"reward at (s=1, a=3,s'=2): \", env.reward_fn(1,3,2))\n",
    "print(\"\")\n",
    "\n",
    "# A random policy\n",
    "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
    "print(\"random policy = \", policy)\n",
    "\n",
    "# Interacting with the environment\n",
    "print(\"(s, a, s', r):\")\n",
    "for time in range(4):\n",
    "    action = policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print(state, action, next_state, reward)\n",
    "    if done:\n",
    "        break\n",
    "    state = next_state\n",
    "print(\"\")\n",
    "\n",
    "# Visualizing the environment\n",
    "try:\n",
    "    print(\"Visualization:\")\n",
    "    env.render()\n",
    "except:\n",
    "    pass # render not available\n",
    "\n",
    "print(env.step(1))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Bellman operator\n",
    "\n",
    "1. Write a function that takes an environment and a state-action value function $Q$ as input and returns the Bellman optimality operator applied to $Q$, $T^* Q$ and the greedy policy with respect to $Q$.\n",
    "3. Let $Q_1$ and $Q_2$ be state-action value functions. Verify the contraction property:  $\\Vert T^* Q_1 - T^* Q_2\\Vert \\leq \\gamma ||Q_1 - Q_2||$, where $||Q|| = \\max_{s,a} |Q(s,a)|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Your answer to 1.\n",
    "# --------------\n",
    "def bellman_operator(Q, env):\n",
    "    TQ = np.zeros((env.Ns, env.Na))\n",
    "    greedy_policy = np.zeros(env.Ns)\n",
    "    for s in env.states:\n",
    "        for a in env.actions:\n",
    "            prob = env.P[s, a, :]\n",
    "            rewards = np.array([float(env.reward_fn(s,a, s_)) for s_ in env.states])\n",
    "            TQ[s,a] = np.sum( prob*(rewards + env.gamma*Q.max(axis=1))  )\n",
    "\n",
    "    argmax = np.argmax(TQ, axis = 1)\n",
    "    greedy_policy = argmax\n",
    "    \n",
    "    return TQ, greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction of Bellman operator: \n",
      "0.5916455856246124\n",
      "0.6736793086199777\n",
      "0.39967742490511116\n",
      "0.8943852829872322\n",
      "0.5185774272425614\n",
      "0.32226252068392547\n",
      "0.4547993934537228\n",
      "0.45923138496643207\n",
      "0.5611003368343374\n",
      "0.6660178931065607\n",
      "0.5612756281494009\n",
      "0.48910149422224564\n",
      "0.7497863508561882\n",
      "0.5848743464003708\n",
      "0.41802366372129474\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Your answer to 2.\n",
    "# --------------\n",
    "n_simulations = 15\n",
    "\n",
    "print(\"Contraction of Bellman operator: \")\n",
    "for ii in range(n_simulations):\n",
    "    Q1 = np.random.randn(env.Ns, env.Na)\n",
    "    Q2 = np.random.randn(env.Ns, env.Na)\n",
    "\n",
    "    # Contraction of Bellman operator\n",
    "    contraction = np.abs(bellman_operator(Q1, env)[0] - bellman_operator(Q2, env)[0]).max() / np.abs(Q1-Q2).max()\n",
    "    print(contraction)\n",
    "    assert contraction <= env.gamma + 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Value iteration (VI)\n",
    "\n",
    "1. (Optimal Value function) Write a function that takes as input an initial state-action value function `Q0` and an environment `env` and returns a vector `Q` such that $||T^* Q -  Q ||_\\infty \\leq \\varepsilon $ and the greedy policy with respect to $Q$.\n",
    "2. Test the convergence of the function you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Your answer to 1.\n",
    "# --------------\n",
    "def value_iteration(Q0, env, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Finding the optimal value function. To be done!\n",
    "    \"\"\"\n",
    "    it = 1\n",
    "    Q = Q0\n",
    "    while True:\n",
    "        TQ, greedy_policy = bellman_operator(Q, env)\n",
    "\n",
    "        err = np.abs(TQ-Q).max() \n",
    "        if err < epsilon:\n",
    "            return TQ, greedy_policy\n",
    "\n",
    "        Q = TQ\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of T(Q) - Q =  9.185378004872291e-07\n",
      "[3 3 3 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2]\n",
      "A  -  -  -  -  + \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "A  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "o  A  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "o  o  A  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "o  o  o  A  o  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "o  o  o  o  A  o \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  + \n",
      "o  o  o  o  o  A \n",
      "o  o  o  o  o  o \n",
      "\n",
      "o  -  -  -  -  A \n",
      "o  o  o  o  o  o \n",
      "o  o  o  o  o  o \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Your answer to 2.\n",
    "# --------------\n",
    "epsilon = 1e-6\n",
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "\n",
    "Q, greedy_policy = value_iteration(Q0, env, epsilon)\n",
    "err = np.abs(Q - bellman_operator(Q, env)[0]).max()\n",
    "print(\"norm of T(Q) - Q = \", err)\n",
    "assert err <= epsilon\n",
    "print(greedy_policy)\n",
    "\n",
    "# simulate optimal policy for some steps\n",
    "state = env.reset()\n",
    "env.render()\n",
    "for t in range(7):\n",
    "    action = greedy_policy[state]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Policy iteration (PI)\n",
    "\n",
    "Policy iteration is another algorithm to find an optimal policy when the MDP is known:\n",
    "\n",
    "$$\n",
    "\\pi_{n} \\gets \\mathrm{greedy}(V_{\\pi_{n-1}}) \\\\\n",
    "V_{\\pi_n} \\gets \\mbox{policy-evaluation}(\\pi_n)\n",
    "$$\n",
    "For any arbitrary $\\pi_0$, $\\pi_n$ converges to $\\pi^*$.\n",
    "\n",
    "Implement policy iteration and compare it to value iteration.\n",
    "\n",
    "\n",
    "### Exact policy evaluation\n",
    "\n",
    "Each iteration of PI requires a policy evaluation. This step can be done with value iteration, but the value of a policy can also be computed exactly by solving a linear system: $(I - \\gamma P_\\pi) V^\\pi = r_\\pi $ where $P_\\pi$ is the transition kernel (matrix) induced by $\\pi$: $P_\\pi(s, s') = P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ and $r_\\pi(s') = \\sum_{s} r(s,\\pi(s),s') P(S_{t+1}=s' | S_t = s, A_t = \\pi(s)) $ is the average reward obtained in state $s'$ by following the policy $\\pi$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_policy_eval(pi, env):\n",
    "    # Compute the transition matrix P_pi and reward vector r_pi\n",
    "    P_pi = np.zeros((env.Ns, env.Ns))\n",
    "    r_pi = np.zeros(env.Ns)\n",
    "    \n",
    "    for s in env.states:\n",
    "        for s_ in env.states:\n",
    "            a = pi[s]\n",
    "            P_pi[s, s_] += env.P[s,a,s_]\n",
    "            r_pi[s] += env.reward_fn(s,a,s_)*env.P[s,a,s_]\n",
    "\n",
    "    A = np.eye(env.Ns) - env.gamma*P_pi\n",
    "    b = r_pi\n",
    "\n",
    "    # Solve linear system\n",
    "    V = np.linalg.solve(A, b)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(pi0, env):\n",
    "    it = 1\n",
    "    policy = pi0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V = exact_policy_eval(policy, env)\n",
    "\n",
    "        # Policy improvement\n",
    "        Q = np.zeros((env.Ns, env.Na))\n",
    "        for s in env.states:\n",
    "            for a in env.actions:\n",
    "                prob = env.P[s, a, :]\n",
    "                rewards = np.array([float(env.reward_fn(s,a, s_)) for s_ in env.states])\n",
    "                Q[s,a] = np.sum( prob*(rewards + env.gamma*V)  )\n",
    "        greedy_policy = Q.argmax(axis=1)\n",
    "        \n",
    "        if np.array_equal(policy, greedy_policy):\n",
    "            print(\"(policy iteration) number of iterations = \", it)\n",
    "            return greedy_policy\n",
    "\n",
    "        policy = greedy_policy\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations =  6\n",
      "[3 3 3 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2]\n",
      "[3 3 3 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "Q0 = np.zeros((env.Ns, env.Na))\n",
    "pi0 = np.random.randint(env.Na, size = (env.Ns,))\n",
    "\n",
    "_, greedy_policy = value_iteration(Q0, env)\n",
    "pi = policy_iteration(pi0, env)\n",
    "\n",
    "print(pi)  # policu returned by policy iteration\n",
    "print(greedy_policy) # greedy policy obtained with value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Q-Learning\n",
    "\n",
    "\n",
    "###  Q-Learning \n",
    "\n",
    "When the reward function and the transition probabilities are *unknown*, we cannot use dynamic programming to find the optimal value function. Q-Learning is a stochastic approximation algorithm that allows us to estimate the value function by using only samples from the environment.\n",
    "\n",
    "Q-learning with $\\varepsilon$-greedy exploration does the following update at time $t$:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$;\n",
    "2. Observe $s_{t+1}$ and reward $r_t$;\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}  $\n",
    "\n",
    "\n",
    "\n",
    "#####  Q-learning\n",
    "\n",
    "The Q-Learning algorithm allows us to estimate the optimal Q function using only trajectories from the MDP obtained by following some exploration policy. \n",
    "\n",
    "Q-learning with $\\varepsilon$-greedy exploration does the following update at time $t$:\n",
    "\n",
    "1. In state $s_t$, take action $a_t$  such that $a_t$ is random with probability $\\varepsilon$ and $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ with probability $1-\\varepsilon$ (**act function**);\n",
    "2. Observe $s_{t+1}$ and reward $r_t$ (**step in the environment**);\n",
    "3. Compute $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$ (**to be done in .update()**) ;\n",
    "4. Update $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$ (**in update() too**)\n",
    "\n",
    "\n",
    "Implement Q-learning and test its convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Q-Learning implementation\n",
    "# ------------------------------\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Implements Q-learning algorithm with epsilon-greedy exploration\n",
    "    \"\"\"\n",
    "    def __init__(self, env, gamma, learning_rate=None, min_learning_rate=0.0, epsilon=1.0, epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01, rmax=1.0, seed=42):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = np.ones((env.Ns, env.Na))*rmax/(1.0-gamma)\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "        self.state = env.reset()\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "    def get_delta(self, r, x, a, y):\n",
    "        \"\"\"\n",
    "        :param r: reward\n",
    "        :param x: current state\n",
    "        :param a: current action\n",
    "        :param y: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        max_q_y_a = self.Q[y, self.env.available_actions()].max()\n",
    "        q_x_a = self.Q[x, a]\n",
    "\n",
    "        return r + self.gamma*max_q_y_a - q_x_a\n",
    "\n",
    "    def get_learning_rate(self, s, a):\n",
    "        if self.learning_rate is None:\n",
    "            return max(1.0/max(1.0, self.Nsa[s, a])**0.8, self.min_learning_rate)\n",
    "        else:\n",
    "            return max(self.learning_rate, self.min_learning_rate)\n",
    "    \n",
    "    def act(self, state, greedy=False): # You don't have to use this template for your algorithm, those are just hints\n",
    "        \"\"\"\n",
    "        Takes a state as input and outputs an action (acting greedily or not with respect to the q function)\n",
    "        \"\"\"\n",
    "        if self.RS.uniform(0, 1) < self.epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(self.env.available_actions())\n",
    "        else:\n",
    "            # exploit\n",
    "            state = self.env.state\n",
    "            actions = self.env.available_actions()\n",
    "            temp = np.max(self.Q[state, actions])\n",
    "            a = np.abs(self.Q[state, :]-temp).argmin()\n",
    "            return a\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Takes (s, a, s', r) as input and update the Q function\n",
    "        \"\"\"\n",
    "        # Current state\n",
    "        x = self.env.state\n",
    "\n",
    "        # Choose action\n",
    "        a = self.act(x)\n",
    "\n",
    "        # Learning rate\n",
    "        alpha = self.get_learning_rate(x, a)\n",
    "\n",
    "        # Take step\n",
    "        observation, reward, done, info = self.env.step(a)\n",
    "        y = observation\n",
    "        r = reward\n",
    "        delta = self.get_delta(r, x, a, y)\n",
    "\n",
    "        # Update\n",
    "        self.Q[x, a] = self.Q[x, a] + alpha*delta\n",
    "\n",
    "        self.Nsa[x, a] += 1\n",
    "        self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "        if done:\n",
    "            self.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(policy iteration) number of iterations =  9\n",
      "optimal value function: [14.70183781 15.47561875 16.290125   18.         20.         20.\n",
      " 15.47561875 16.290125   17.1475     18.05       19.         20.\n",
      " 14.70183781 15.47561875 16.290125   17.1475     18.05       19.        ]\n",
      "q learning estimate:  [14.70314975 15.81032404 16.48738376 18.         20.         20.\n",
      " 15.47588145 16.29016822 17.14750501 18.05000029 19.         20.\n",
      " 15.8156555  16.06615545 16.53638802 17.22709206 18.05601478 19.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Convergence of Q-Learning\n",
    "# ---------------------------\n",
    "\n",
    "# Number of Q learning iterations\n",
    "n_steps = int(1e4)  \n",
    "\n",
    "# Policy iteration and evaluation (to get exact optimal V)\n",
    "pi0 = np.zeros(env.Ns, dtype=np.int32)  # initial policy\n",
    "pi_opt = policy_iteration(pi0, env)\n",
    "v_opt = exact_policy_eval(pi_opt, env)\n",
    "\n",
    "# Create qlearning object\n",
    "qlearning = QLearning(env, gamma=env.gamma, learning_rate=None)\n",
    "\n",
    "# Run Q Learning\n",
    "tt = 0\n",
    "v_list = np.zeros((n_steps, env.Ns))\n",
    "while tt < n_steps:\n",
    "    qlearning.step()\n",
    "    # Estimate V*\n",
    "    v_star_est = qlearning.Q.max(axis=1)\n",
    "    v_list[tt, :] = v_star_est\n",
    "    tt +=1\n",
    "\n",
    "# Compute greedy policy\n",
    "greedy_policy = np.argmax(qlearning.Q, axis=1)\n",
    "\n",
    "# Plot\n",
    "diff = v_list - v_opt\n",
    "norm_diff = np.linalg.norm(diff, axis=1)\n",
    "plt.plot(norm_diff)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.title(\"Q-learning convergence\")\n",
    "\n",
    "print(\"optimal value function:\", v_opt)\n",
    "print(\"q learning estimate: \", v_list[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
